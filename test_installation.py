#!/usr/bin/env python3
"""
å¿«é€Ÿå®‰è£…éªŒè¯è„šæœ¬

ç”¨æ³•ï¼špython test_installation.py

è¿™æ˜¯ä¸€ä¸ªç®€åŒ–ç‰ˆçš„è¯Šæ–­è„šæœ¬ï¼Œåªæ£€æŸ¥å…³é”®ç»„ä»¶æ˜¯å¦æ­£å¸¸å·¥ä½œã€‚
å¦‚éœ€å®Œæ•´è¯Šæ–­ï¼Œè¯·è¿è¡Œï¼špython diagnose.py
"""

import sys

def test_import(module_name, from_module=None):
    """æµ‹è¯•æ¨¡å—å¯¼å…¥"""
    try:
        if from_module:
            exec(f"from {module_name} import {from_module}")
            print(f"  âœ… {module_name}.{from_module}")
        else:
            __import__(module_name)
            print(f"  âœ… {module_name}")
        return True
    except ImportError as e:
        print(f"  âŒ {module_name}: {e}")
        return False

def main():
    print("="*60)
    print("  å¿«é€Ÿå®‰è£…éªŒè¯")
    print("="*60)
    print()

    all_ok = True

    # åŸºç¡€ä¾èµ–
    print("æ£€æŸ¥åŸºç¡€ä¾èµ–...")
    all_ok &= test_import("torch")
    all_ok &= test_import("numpy")
    all_ok &= test_import("PIL", "Image")
    all_ok &= test_import("cv2")
    print()

    # å…³é”®ä¾èµ–
    print("æ£€æŸ¥å…³é”®ä¾èµ–...")
    all_ok &= test_import("transformers")
    all_ok &= test_import("timm")
    all_ok &= test_import("einops")
    all_ok &= test_import("loguru")
    print()

    # ç‰ˆæœ¬æ£€æŸ¥
    print("æ£€æŸ¥ç‰ˆæœ¬...")
    try:
        import transformers
        version = transformers.__version__
        print(f"  transformers: {version}")
        if version == "4.38.0":
            print(f"  âš ï¸  ç‰ˆæœ¬4.38.0ä¸åŒ…å«Florence2ï¼Œéœ€è¦å‡çº§åˆ°4.38.1+")
            all_ok = False
        elif version < "4.38.1":
            print(f"  âŒ ç‰ˆæœ¬è¿‡ä½ï¼Œéœ€è¦>=4.38.1")
            all_ok = False
        else:
            print(f"  âœ… ç‰ˆæœ¬æ»¡è¶³è¦æ±‚")
    except:
        print(f"  âŒ æ— æ³•æ£€æŸ¥transformersç‰ˆæœ¬")
        all_ok = False
    print()

    # æ¨¡å‹å¯¼å…¥
    print("æ£€æŸ¥æ¨¡å‹å¯¼å…¥...")
    all_ok &= test_import("transformers", "Florence2ForConditionalGeneration")
    all_ok &= test_import("transformers", "AutoProcessor")
    all_ok &= test_import("iopaint.model_manager", "ModelManager")
    print()

    # æ¨¡å‹æ–‡ä»¶
    print("æ£€æŸ¥æ¨¡å‹æ–‡ä»¶...")
    from pathlib import Path
    lama_path = Path.home() / ".cache" / "torch" / "hub" / "checkpoints" / "big-lama.pt"
    if lama_path.exists():
        size_mb = lama_path.stat().st_size / (1024 * 1024)
        if 190 < size_mb < 200:
            print(f"  âœ… LaMAæ¨¡å‹: {size_mb:.1f}MB")
        else:
            print(f"  âš ï¸  LaMAæ¨¡å‹å¤§å°å¼‚å¸¸: {size_mb:.1f}MB (åº”çº¦196MB)")
            all_ok = False
    else:
        print(f"  âŒ LaMAæ¨¡å‹æœªæ‰¾åˆ°: {lama_path}")
        all_ok = False
    print()

    # GPUæ£€æŸ¥
    print("æ£€æŸ¥GPUæ”¯æŒ...")
    try:
        import torch
        if torch.cuda.is_available():
            print(f"  âœ… CUDAå¯ç”¨: {torch.version.cuda}")
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            print(f"  âœ… MPS (Apple GPU) å¯ç”¨")
        else:
            print(f"  âš ï¸  ä»…CPUå¯ç”¨ï¼ˆå¤„ç†ä¼šè¾ƒæ…¢ï¼‰")
    except:
        print(f"  âŒ æ— æ³•æ£€æŸ¥GPU")
    print()

    # ç»“æœ
    print("="*60)
    if all_ok:
        print("  ğŸ‰ å®‰è£…éªŒè¯é€šè¿‡ï¼")
        print()
        print("  ä¸‹ä¸€æ­¥:")
        print("    1. é‡å¯ ComfyUI")
        print("    2. åœ¨èŠ‚ç‚¹èœå•ä¸­æ‰¾åˆ°: JM-Nodes â†’ Video â†’ Sora")
        print("    3. ä½¿ç”¨ Sora Watermark Remover èŠ‚ç‚¹")
    else:
        print("  âš ï¸  å‘ç°ä¸€äº›é—®é¢˜")
        print()
        print("  å»ºè®®:")
        print("    1. è¿è¡Œå®Œæ•´è¯Šæ–­: python diagnose.py")
        print("    2. è¿è¡Œè‡ªåŠ¨ä¿®å¤: bash fix_dependencies.sh")
        print("    3. æŸ¥çœ‹æ–‡æ¡£: QUICK_START.md")
    print("="*60)

    return 0 if all_ok else 1

if __name__ == "__main__":
    sys.exit(main())
