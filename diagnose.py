#!/usr/bin/env python3
"""
ComfyUI-JM-Sora-Watermark-Remover ç»¼åˆè¯Šæ–­å·¥å…·

ç”¨æ³•ï¼š
    python diagnose.py

åŠŸèƒ½ï¼š
    - æ£€æµ‹Pythonç¯å¢ƒå’Œæ¶æ„
    - æ£€æµ‹æ‰€æœ‰ä¾èµ–ç‰ˆæœ¬
    - éªŒè¯æ¨¡å‹æ˜¯å¦å¯ç”¨
    - æ£€æµ‹GPU/MPSåŠ é€Ÿ
    - æä¾›é’ˆå¯¹æ€§çš„ä¿®å¤å»ºè®®
"""

import sys
import platform
import subprocess
from pathlib import Path

def print_header(title):
    """æ‰“å°ç« èŠ‚æ ‡é¢˜"""
    print("\n" + "="*60)
    print(f"  {title}")
    print("="*60 + "\n")

def print_ok(message):
    """æ‰“å°æˆåŠŸæ¶ˆæ¯"""
    print(f"âœ… {message}")

def print_warning(message):
    """æ‰“å°è­¦å‘Šæ¶ˆæ¯"""
    print(f"âš ï¸  {message}")

def print_error(message):
    """æ‰“å°é”™è¯¯æ¶ˆæ¯"""
    print(f"âŒ {message}")

def check_system():
    """æ£€æŸ¥ç³»ç»Ÿä¿¡æ¯"""
    print_header("ç³»ç»Ÿä¿¡æ¯")

    system = platform.system()
    machine = platform.machine()
    python_version = sys.version.split()[0]

    print(f"æ“ä½œç³»ç»Ÿ: {system}")
    print(f"CPUæ¶æ„: {machine}")
    print(f"Pythonç‰ˆæœ¬: {python_version}")
    print(f"Pythonè·¯å¾„: {sys.executable}")

    # æ£€æŸ¥Pythonæ¶æ„
    result = subprocess.run(['file', sys.executable], capture_output=True, text=True)

    issues = []

    if system == "Darwin":  # macOS
        if 'x86_64' in result.stdout and machine == 'arm64':
            print_error("Pythonæ¶æ„: Intel x86_64 (Rosetta 2)")
            print("   ç³»ç»Ÿæ˜¯ARM64 (M1/M2/M3) ä½†Pythonæ˜¯Intelç‰ˆæœ¬")
            print("   æ€§èƒ½æŸå¤±: çº¦30-50%")
            issues.append({
                'type': 'architecture',
                'severity': 'high',
                'message': 'Pythonè¿è¡Œåœ¨Rosetta 2ä¸‹',
                'fix': 'install_arm64_python'
            })
        elif 'arm64' in result.stdout:
            print_ok("Pythonæ¶æ„: ARM64 (åŸç”Ÿ)")
        else:
            print_warning(f"æœªçŸ¥æ¶æ„: {result.stdout}")
    elif system == "Linux":
        print_ok(f"Pythonæ¶æ„: {machine}")

    return issues

def check_dependencies():
    """æ£€æŸ¥Pythonä¾èµ–"""
    print_header("ä¾èµ–ç‰ˆæœ¬æ£€æŸ¥")

    issues = []

    # å¿…éœ€çš„åŒ…å’Œæœ€ä½ç‰ˆæœ¬
    required = {
        'torch': '2.0.0',
        'transformers': '4.38.1',  # æ³¨æ„ä¸æ˜¯4.38.0
        'timm': '0.9.0',
        'einops': '0.7.0',
        'opencv-python': '4.8.0',
        'PIL': '10.0.0',
        'loguru': None,
        'iopaint': None,
    }

    for package, min_version in required.items():
        package_name = package
        if package == 'PIL':
            package_name = 'Pillow'
        elif package == 'opencv-python':
            package_name = 'cv2'

        try:
            if package_name == 'cv2':
                import cv2
                version = cv2.__version__
            elif package_name == 'Pillow':
                from PIL import Image
                import PIL
                version = PIL.__version__
            else:
                mod = __import__(package_name)
                version = mod.__version__

            # ç‰ˆæœ¬æ¯”è¾ƒ
            if min_version:
                from packaging import version as pkg_version
                if pkg_version.parse(version) >= pkg_version.parse(min_version):
                    print_ok(f"{package}: {version} (>= {min_version})")
                else:
                    print_error(f"{package}: {version} (éœ€è¦ >= {min_version})")
                    issues.append({
                        'type': 'dependency',
                        'severity': 'critical',
                        'package': package,
                        'current': version,
                        'required': min_version,
                        'fix': 'upgrade_package'
                    })
            else:
                print_ok(f"{package}: {version}")

        except ImportError:
            print_error(f"{package}: æœªå®‰è£…")
            issues.append({
                'type': 'dependency',
                'severity': 'critical',
                'package': package,
                'current': None,
                'required': min_version,
                'fix': 'install_package'
            })

    # ç‰¹åˆ«æ£€æŸ¥transformersæ˜¯å¦ä¸º4.38.0
    try:
        import transformers
        if transformers.__version__ == '4.38.0':
            print_error("transformers: 4.38.0 (æ­¤ç‰ˆæœ¬ä¸åŒ…å«Florence2!)")
            print("   å¿…é¡»å‡çº§åˆ° 4.38.1 æˆ–æ›´é«˜ç‰ˆæœ¬")
            issues.append({
                'type': 'dependency',
                'severity': 'critical',
                'package': 'transformers',
                'current': '4.38.0',
                'required': '4.38.1',
                'fix': 'upgrade_transformers_438'
            })
    except:
        pass

    return issues

def check_florence2():
    """æ£€æŸ¥Florence-2æ˜¯å¦å¯å¯¼å…¥"""
    print_header("Florence-2 æ¨¡å‹æ£€æŸ¥")

    issues = []

    try:
        from transformers import Florence2ForConditionalGeneration, AutoProcessor
        print_ok("Florence2ForConditionalGeneration å¯ä»¥å¯¼å…¥")
        print_ok("AutoProcessor å¯ä»¥å¯¼å…¥")
    except ImportError as e:
        print_error(f"æ— æ³•å¯¼å…¥Florence-2: {e}")
        issues.append({
            'type': 'model',
            'severity': 'critical',
            'message': 'Florence-2æ— æ³•å¯¼å…¥',
            'fix': 'upgrade_transformers'
        })

    return issues

def check_lama():
    """æ£€æŸ¥LaMAæ¨¡å‹"""
    print_header("LaMA æ¨¡å‹æ£€æŸ¥")

    issues = []

    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    lama_path = Path.home() / ".cache" / "torch" / "hub" / "checkpoints" / "big-lama.pt"

    if lama_path.exists():
        size_mb = lama_path.stat().st_size / (1024 * 1024)
        if size_mb > 190 and size_mb < 200:
            print_ok(f"LaMAæ¨¡å‹æ–‡ä»¶å­˜åœ¨: {lama_path}")
            print(f"   æ–‡ä»¶å¤§å°: {size_mb:.1f} MB")
        else:
            print_warning(f"LaMAæ¨¡å‹æ–‡ä»¶å¤§å°å¼‚å¸¸: {size_mb:.1f} MB (åº”è¯¥çº¦196MB)")
            issues.append({
                'type': 'model',
                'severity': 'medium',
                'message': 'LaMAæ¨¡å‹æ–‡ä»¶å¯èƒ½æŸå',
                'fix': 'redownload_lama'
            })
    else:
        print_error(f"LaMAæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {lama_path}")
        issues.append({
            'type': 'model',
            'severity': 'critical',
            'message': 'LaMAæ¨¡å‹æœªä¸‹è½½',
            'fix': 'download_lama'
        })

    # æ£€æŸ¥IOPaint
    try:
        from iopaint.model_manager import ModelManager
        print_ok("IOPaint å¯ä»¥å¯¼å…¥")
    except ImportError as e:
        print_error(f"IOPaint æ— æ³•å¯¼å…¥: {e}")
        issues.append({
            'type': 'dependency',
            'severity': 'critical',
            'package': 'iopaint',
            'fix': 'install_iopaint'
        })

    return issues

def check_gpu():
    """æ£€æŸ¥GPUåŠ é€Ÿ"""
    print_header("GPU åŠ é€Ÿæ£€æŸ¥")

    issues = []

    try:
        import torch

        # CUDA
        if torch.cuda.is_available():
            print_ok(f"CUDA å¯ç”¨")
            print(f"   CUDAç‰ˆæœ¬: {torch.version.cuda}")
            print(f"   GPUæ•°é‡: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                print(f"   GPU {i}: {torch.cuda.get_device_name(i)}")
        else:
            print_warning("CUDA ä¸å¯ç”¨")

        # MPS (Apple Silicon)
        if hasattr(torch.backends, 'mps'):
            if torch.backends.mps.is_available():
                print_ok("MPS (Apple GPU) å¯ç”¨")

                # æ€§èƒ½æµ‹è¯•
                try:
                    import time
                    size = 1000
                    cpu_tensor = torch.randn(size, size)
                    mps_tensor = cpu_tensor.to('mps')

                    start = time.time()
                    for _ in range(10):
                        _ = torch.matmul(cpu_tensor, cpu_tensor)
                    cpu_time = time.time() - start

                    start = time.time()
                    for _ in range(10):
                        _ = torch.matmul(mps_tensor, mps_tensor)
                    torch.mps.synchronize()
                    mps_time = time.time() - start

                    speedup = cpu_time / mps_time

                    if speedup > 1.2:
                        print_ok(f"   MPSæ€§èƒ½æµ‹è¯•: {speedup:.2f}x åŠ é€Ÿ")
                    elif speedup < 0.8:
                        print_warning(f"   MPSæ€§èƒ½æµ‹è¯•: {speedup:.2f}x (æ¯”CPUæ…¢!)")
                        print("   è¿™é€šå¸¸è¡¨ç¤ºPythonè¿è¡Œåœ¨Rosetta 2ä¸‹")
                        issues.append({
                            'type': 'performance',
                            'severity': 'high',
                            'message': 'MPSæ€§èƒ½å¼‚å¸¸',
                            'fix': 'check_architecture'
                        })
                    else:
                        print_warning(f"   MPSæ€§èƒ½æµ‹è¯•: {speedup:.2f}x (æ€§èƒ½ä¸€èˆ¬)")

                except Exception as e:
                    print_warning(f"   MPSæ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
            else:
                print_warning("MPS ä¸å¯ç”¨")

        # è®¾å¤‡é€‰æ‹©
        if torch.cuda.is_available():
            device = "cuda"
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            device = "mps"
        else:
            device = "cpu"

        print(f"\nå½“å‰ä¼šä½¿ç”¨çš„è®¾å¤‡: {device}")

        if device == "cpu":
            print_warning("å°†ä½¿ç”¨CPUå¤„ç†ï¼Œé€Ÿåº¦ä¼šå¾ˆæ…¢")
            print("å»ºè®®ï¼š")
            if platform.system() == "Darwin":
                print("  - ç¡®ä¿å®‰è£…äº†æ”¯æŒMPSçš„PyTorchç‰ˆæœ¬")
                print("  - ä½¿ç”¨ARM64åŸç”ŸPythonä»¥è·å¾—æ›´å¥½çš„MPSæ”¯æŒ")
            else:
                print("  - å®‰è£…CUDAç‰ˆæœ¬çš„PyTorchä»¥ä½¿ç”¨GPUåŠ é€Ÿ")

    except ImportError:
        print_error("PyTorch æœªå®‰è£…")
        issues.append({
            'type': 'dependency',
            'severity': 'critical',
            'package': 'torch',
            'fix': 'install_pytorch'
        })

    return issues

def generate_fix_commands(issues):
    """ç”Ÿæˆä¿®å¤å‘½ä»¤"""
    if not issues:
        print_header("è¯Šæ–­ç»“æœ")
        print_ok("æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼ç¯å¢ƒé…ç½®æ­£ç¡®ã€‚")
        return

    print_header("å‘ç°çš„é—®é¢˜")

    critical = [i for i in issues if i.get('severity') == 'critical']
    high = [i for i in issues if i.get('severity') == 'high']
    medium = [i for i in issues if i.get('severity') == 'medium']

    if critical:
        print(f"\nğŸ”´ ä¸¥é‡é—®é¢˜ ({len(critical)}ä¸ª):")
        for issue in critical:
            if issue['type'] == 'dependency':
                pkg = issue.get('package', 'æœªçŸ¥')
                current = issue.get('current', 'æœªå®‰è£…')
                required = issue.get('required', 'æœªçŸ¥')
                print(f"  - {pkg}: {current} (éœ€è¦ >= {required})")
            else:
                print(f"  - {issue.get('message', 'æœªçŸ¥é—®é¢˜')}")

    if high:
        print(f"\nâš ï¸  é«˜ä¼˜å…ˆçº§é—®é¢˜ ({len(high)}ä¸ª):")
        for issue in high:
            print(f"  - {issue.get('message', 'æœªçŸ¥é—®é¢˜')}")

    if medium:
        print(f"\nâš ï¸  ä¸­ç­‰é—®é¢˜ ({len(medium)}ä¸ª):")
        for issue in medium:
            print(f"  - {issue.get('message', 'æœªçŸ¥é—®é¢˜')}")

    print_header("ä¿®å¤å»ºè®®")

    # æ£€æŸ¥æ˜¯å¦æ˜¯æ¶æ„é—®é¢˜
    arch_issues = [i for i in issues if i.get('type') == 'architecture']
    if arch_issues:
        print("\nğŸ“‹ æ­¥éª¤1: ä¿®å¤Pythonæ¶æ„é—®é¢˜ (æ¨è)")
        print("â”€" * 60)
        print("å½“å‰Pythonæ˜¯Intelç‰ˆæœ¬ï¼Œå»ºè®®å®‰è£…ARM64åŸç”Ÿç‰ˆæœ¬è·å¾—æœ€ä½³æ€§èƒ½ã€‚")
        print("\nå®‰è£…ARM64ç‰ˆæœ¬çš„Miniforge:")
        print("```bash")
        print("curl -L -O https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-MacOSX-arm64.sh")
        print("bash Miniforge3-MacOSX-arm64.sh")
        print("```")
        print("\næˆ–è€…æš‚æ—¶ä½¿ç”¨Intelç‰ˆæœ¬ï¼Œä½†éœ€è¦ä¼˜åŒ–å‚æ•°:")
        print("åœ¨ComfyUIä¸­è®¾ç½® detection_skip=5 æ¥æå‡é€Ÿåº¦")

    # æ£€æŸ¥ä¾èµ–é—®é¢˜
    dep_issues = [i for i in issues if i.get('type') == 'dependency']
    if dep_issues:
        step_num = 2 if arch_issues else 1
        print(f"\nğŸ“‹ æ­¥éª¤{step_num}: ä¿®å¤ä¾èµ–é—®é¢˜")
        print("â”€" * 60)

        # ç”Ÿæˆpipå‘½ä»¤
        to_upgrade = []
        to_install = []

        for issue in dep_issues:
            pkg = issue.get('package')
            if pkg:
                if issue.get('current'):
                    to_upgrade.append(pkg)
                else:
                    to_install.append(pkg)

        if to_upgrade or to_install:
            print("è¿è¡Œä»¥ä¸‹å‘½ä»¤ä¿®å¤ä¾èµ–:")
            print("```bash")

            # ç‰¹åˆ«å¤„ç†transformers 4.38.0
            if any(i.get('fix') == 'upgrade_transformers_438' for i in dep_issues):
                print("# å‡çº§transformers (4.38.0ä¸åŒ…å«Florence2ï¼Œå¿…é¡»å‡çº§)")
                print("pip install --upgrade transformers==4.57.3")
                print()

            if 'transformers' in to_upgrade and not any(i.get('fix') == 'upgrade_transformers_438' for i in dep_issues):
                print("# å‡çº§transformersåˆ°æœ€æ–°ç‰ˆæœ¬")
                print("pip install --upgrade transformers>=4.38.1")
                print()

            other_upgrades = [p for p in to_upgrade if p != 'transformers']
            if other_upgrades:
                print("# å‡çº§å…¶ä»–ä¾èµ–")
                for pkg in other_upgrades:
                    print(f"pip install --upgrade {pkg}")
                print()

            if to_install:
                print("# å®‰è£…ç¼ºå¤±çš„ä¾èµ–")
                for pkg in to_install:
                    if pkg == 'iopaint':
                        print("pip install iopaint --no-deps")
                    else:
                        print(f"pip install {pkg}")
                print()

            print("# æˆ–ä¸€é”®å®‰è£…æ‰€æœ‰ä¾èµ–")
            print("cd /path/to/ComfyUI-JM-Sora-Watermark-Remover")
            print("python install.py")
            print("```")

    # æ£€æŸ¥æ¨¡å‹é—®é¢˜
    model_issues = [i for i in issues if i.get('type') == 'model']
    if model_issues:
        step_num = len([arch_issues, dep_issues]) + 1 if (arch_issues or dep_issues) else 1
        print(f"\nğŸ“‹ æ­¥éª¤{step_num}: ä¸‹è½½æ¨¡å‹")
        print("â”€" * 60)
        print("è¿è¡Œå®‰è£…è„šæœ¬ä¸‹è½½æ‰€éœ€æ¨¡å‹:")
        print("```bash")
        print("cd /path/to/ComfyUI-JM-Sora-Watermark-Remover")
        print("python install.py")
        print("```")

    print("\n" + "="*60)
    print("ä¿®å¤å®Œæˆåï¼Œé‡å¯ComfyUIå¹¶é‡æ–°è¿è¡Œæ­¤è¯Šæ–­è„šæœ¬éªŒè¯ã€‚")
    print("="*60)

def main():
    """ä¸»å‡½æ•°"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ComfyUI-JM-Sora-Watermark-Remover ç¯å¢ƒè¯Šæ–­å·¥å…·             â•‘
â•‘  ç‰ˆæœ¬: 1.0                                                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    all_issues = []

    # æ‰§è¡Œæ‰€æœ‰æ£€æŸ¥
    all_issues.extend(check_system())
    all_issues.extend(check_dependencies())
    all_issues.extend(check_florence2())
    all_issues.extend(check_lama())
    all_issues.extend(check_gpu())

    # ç”Ÿæˆä¿®å¤å»ºè®®
    generate_fix_commands(all_issues)

    # è¿”å›çŠ¶æ€ç 
    if any(i.get('severity') == 'critical' for i in all_issues):
        sys.exit(1)
    elif all_issues:
        sys.exit(2)
    else:
        sys.exit(0)

if __name__ == "__main__":
    main()
